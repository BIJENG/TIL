# 📌 Boosting 정리 (AdaBoost → Gradient Boosting → 현대 3대장)

> 목적  
> - Boosting의 흐름과 차이를 구조적으로 이해  
> - AdaBoost와 Gradient Boosting의 혼동 포인트 정리  
> - XGBoost / LightGBM / CatBoost의 역할 구분  

---

## 1️⃣ Ensemble 복습 (한 줄 요약)

**Ensemble**  
> 여러 개의 약한 모델(Weak Learner)을 조합해  
> 하나의 강한 모델(Strong Learner)을 만드는 전략

Ensemble의 대표 전략:
- **Bagging** → Variance 감소
- **Boosting** → Bias 감소

---

## 2️⃣ Boosting이란?

> **이전 모델의 실수를 다음 모델이 보완하며  
> 순차적으로 학습하는 앙상블 기법**

- 병렬 ❌
- 순차 ⭕
- “틀린 걸 다시 공부”하는 방식

---

## 3️⃣ AdaBoost (1세대 Boosting)

### 핵심 아이디어
> **잘못 분류된 샘플에 가중치를 부여해  
> 다음 모델이 그 데이터를 더 집중 학습**

### 특징
- 샘플 가중치 중심
- Decision Stump (깊이 1 트리) 사용
- 노이즈·이상치에 민감
- 요즘은 주로 개념 학습용

### 핵심 키워드
```text
오답 샘플 가중치 ↑
```

## 4️⃣ Gradient Boosting (2세대 Boosting)
### 핵심 아이디어

> **손실함수(Loss Function)를 줄이는 방향으로
> 잔차(residual)를 다음 모델이 학습**

### AdaBoost와의 차이
| 구분            | AdaBoost  | Gradient Boosting |
| ------------- | --------- | ----------------- |
| 기준            | 오답 여부     | 손실함수              |
| 핵심            | 샘플 가중치    | 잔차(residual)      |
| 학습 방향         | 틀린 데이터 강조 | loss 최소화          |
| learning rate | 중요도 낮음    | 핵심 파라미터           |
loss → residual → 다음 모델 보정

## 5️⃣ 헷갈렸던 포인트 정리 ⭐
### ❓ Q1. AdaBoost는 가중치, Gradient Boosting은 평가지표?
❌ 틀린 표현
⭕ 정확한 표현:
AdaBoost → 샘플 가중치
Gradient Boosting → 손실함수(Loss)
👉 평가지표(accuracy, RMSE 등)는 학습이 아니라 평가용

### ❓ Q2. learning rate는 AdaBoost에 쓰는 거 아닌가?
❌ 아님
⭕ learning rate는 Gradient Boosting의 핵심 개념
- Gradient Boosting은
- 잔차를 얼마나 세게 반영할지 조절 필요
- → learning rate 등장
learning_rate ↓ → 안정적, 느림
learning_rate ↑ → 빠르나 과적합 위험

## 6️⃣ 왜 Boosting에서는 대부분 DecisionTree를 쓰나?
개별 성능이 아니라
앙상블에 잘 섞이기 때문
### DecisionTree가 적합한 이유
- 전처리 거의 불필요
- 비선형 관계 자동 학습
- 불안정(Variance 큼) → 앙상블 효과 극대화
- 약하게도, 적당히도 조절 가능
👉 “최고의 단일 모델”이 아니라
“최고의 부품”

## 7️⃣ 현대 Boosting 3대장 (3세대)
### ① XGBoost
Gradient Boosting + 정규화
성능 최강
Kaggle 대회 표준
장점: 성능
단점: 느림, 튜닝 어려움

### ② LightGBM
Leaf-wise 트리 성장
대용량 데이터에 최적
학습 속도 매우 빠름
장점: 속도
단점: 과적합 위험

### ③ CatBoost
범주형 변수 자동 처리
Target leakage 방지
인코딩 거의 불필요
장점: 범주형 데이터
단점: 느림

## 8️⃣ Boosting 모델 선택 가이드
### 빠른 베이스라인
→ Gradient Boosting

### 성능 최우선
→ XGBoost

### 데이터 큼
→ LightGBM

### 범주형 변수 많음
→ CatBoost

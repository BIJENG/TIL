# 🏥 헬스케어 AI/NLP 학습 로드맵 및 구현 가이드

이 저장소는 헬스케어 텍스트 데이터 분석을 위한 자연어 처리(NLP)의 전체 파이프라인과 핵심 개념, 그리고 이를 구현한 파이썬 코드를 정리한 문서입니다. 기초적인 빈도 분석부터 최신 LLM 프롬프트 엔지니어링까지의 학습 흐름을 담고 있습니다.

---

## 📑 목차
1. [AI와 NLP의 계층 구조](#1-ai와-nlp의-계층-구조)
2. [데이터 전처리 (Preprocessing)](https://www.google.com/search?q=%232-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC-preprocessing-%EA%B0%80%EC%9E%A5-%EC%A4%91%EC%9A%94)
3. [통계적 텍스트 분석 (Analysis)](https://www.google.com/search?q=%233-%ED%86%B5%EA%B3%84%EC%A0%81-%ED%85%8D%EC%8A%A4%ED%8A%B8-%EB%B6%84%EC%84%9D-analysis)
4. [딥러닝 모델의 진화 (Model Architecture)](https://www.google.com/search?q=%234-%EB%94%A5%EB%9F%AC%EB%8B%9D-%EB%AA%A8%EB%8D%B8%EC%9D%98-%EC%A7%84%ED%99%94-model-architecture)
5. [LLM의 두 가지 갈래 (BERT vs GPT)](https://www.google.com/search?q=%235-llm%EC%9D%98-%EB%91%90-%EA%B0%80%EC%A7%80-%EA%B0%88%EB%9E%98-bert-vs-gpt)
6. [엔지니어링 및 최적화 (Engineering)](https://www.google.com/search?q=%236-%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-%EB%B0%8F-%EC%B5%9C%EC%A0%81%ED%99%94-engineering)

---

## 1. AI와 NLP의 계층 구조

가장 먼저 내가 다루는 기술이 어디에 속하는지 명확히 정의해야 합니다.

* **개념**:
    * **AI (인공지능)**: 인간의 지능을 모방하는 포괄적 개념.
    * **ML (머신러닝)**: 데이터를 통해 스스로 학습하는 AI의 하위 분야.
    * **NLP (자연어 처리)**: 인간의 언어를 컴퓨터가 이해하게 만드는 AI와 ML의 교집합.
* **헬스케어 적용**: 진료 차트(Text)를 분석하여 질병을 예측(ML)하는 AI 시스템 구축.

```python
# 라이브러리 계층 구조 예시
import tensorflow as tf       # DL (딥러닝)
import scikit-learn as sklearn # ML (머신러닝)
import konlpy                 # NLP (자연어 처리 - 한국어)
```

---

## 2. 데이터 전처리 (Preprocessing): 가장 중요

모델 성능의 80%는 여기서 결정됩니다. 특히 헬스케어 데이터는 전문 용어 보존이 핵심입니다.

### 2-1. 토큰화 (Tokenization) 전략
* **개념**: 문장을 컴퓨터가 이해할 수 있는 작은 조각(Token)으로 나누는 과정.
* **종류 및 차이**:
    1.  **형태소 분석 (Morphological)**: 문법(의미) 단위로 쪼갬. 한국어 의료 데이터 분석의 필수.
    2.  **BPE / WordPiece**: 통계적 빈도/확률 기반. 신조어나 복잡한 약물 이름 처리에 강함 (LLM 방식).

```python
from konlpy.tag import Okt
from transformers import AutoTokenizer

text = "환자는 고혈당 증세를 보임"

# 1. 형태소 분석 (의미 파악 위주)
okt = Okt()
print(f"형태소: {okt.morphs(text)}") 
# 결과: ['환자', '는', '고혈당', '증세', '를', '보임'] -> 조사 분리 가능

# 2. BPE/WordPiece (최신 LLM 위주)
tokenizer = AutoTokenizer.from_pretrained("bert-base-multilingual-cased")
print(f"WordPiece: {tokenizer.tokenize(text)}")
# 결과: ['환자', '##는', '고', '##혈', '##당', ...] -> 데이터 효율성 중시
```

---

## 3. 통계적 텍스트 분석 (Analysis)

데이터의 전체적인 경향성을 파악하는 비지도 학습 단계입니다.

### 3-1. 단어 빈도 및 TF-IDF
* **개념**: 단순히 많이 나온 단어(Frequency)가 아닌, 해당 문서에서만 특별한 단어(TF-IDF)를 찾음.
* **헬스케어 적용**: 모든 환자에게 있는 '내원함' 같은 단어는 무시하고, 특정 환자의 '심근경색' 키워드 추출.

### 3-2. 토픽 모델링 (LDA)
* **개념**: 문서 집합에 잠재된(Latent) 주제(Topic)를 확률적으로 추론.
* **평가 지표**:
    * **Perplexity (혼란도)**: 낮을수록 좋음 (모델이 예측을 잘함).
    * **Coherence (일관성)**: 높을수록 좋음 (단어들이 의미적으로 잘 뭉침). **해석 가능성이 더 중요.**

```python
from gensim import corpora
from gensim.models import LdaModel

# 1. 사전 및 코퍼스 생성
dictionary = corpora.Dictionary(tokenized_docs)
corpus = [dictionary.doc2bow(text) for text in tokenized_docs]

# 2. LDA 모델 학습 (토픽 수 K=3 가정)
lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=3)

# 3. 토픽 확인
print(lda_model.print_topics())
```

---

## 4. 딥러닝 모델의 진화 (Model Architecture)

NLP 기술이 병목 현상을 해결하며 발전해 온 역사입니다.

1.  **RNN (순차 처리)**:
    * 한 방향으로 순서대로 읽음.
    * **한계**: 문장이 길어지면 앞의 내용(과거 병력)을 잊어버림 (Vanishing Gradient).
2.  **Attention (집중)**:
    * RNN을 돕기 위해 중요한 단어를 다시 돌아봄.
3.  **Transformer (병렬 처리)**:
    * **혁신**: 순서를 없애고 모든 단어를 **한꺼번에(Parallel)** 펼쳐 놓고 관계를 계산.
    * **결과**: 대량의 의료 데이터를 빠르게 학습 가능해짐.

---

## 5. LLM의 두 가지 갈래 (BERT vs GPT)

트랜스포머 아키텍처를 어떻게 사용하느냐에 따라 갈립니다.

### 5-1. BERT (Encoder Only)
* **방식**: "환자는 [MASK] 수치가 높다" -> 빈칸 맞히기 (Masked LM).
* **특징**: 양방향 문맥 이해에 강함.
* **용도**: 질병 **분류**, 개체명 인식(NER). (이해 전문가)

### 5-2. GPT / Gemini (Decoder Only)
* **방식**: "환자는 혈당 수치가..." -> 다음 단어 예측 (Causal LM).
* **특징**: 자연스러운 문장 생성에 강함. 데이터가 많아질수록 성능이 급격히 향상됨(Scaling Law).
* **용도**: 의료 상담, 차트 **요약**, 생성. (창작 전문가)

```python
# BERT 방식 (분류/이해)
def predict_mask(text):
    # 입력: "환자는 [MASK]이 있다" -> 출력: "암"
    pass

# GPT 방식 (생성)
def generate_next(text):
    # 입력: "환자 상태는" -> 출력: "안정적입니다."
    pass
```

---

## 6. 엔지니어링 및 최적화 (Engineering)

최신 AI를 실무에서 효과적으로 다루기 위한 기술입니다.

### 6-1. 컨텍스트 윈도우 (Context Window)
* **개념**: AI가 한 번에 기억할 수 있는 대화의 길이(Token Limit).
* **문제**: 환자의 10년 치 기록이 윈도우를 넘어가면 AI는 과거를 잊음.
* **해결**: RAG(검색 증강 생성) 기술 도입.

### 6-2. 프롬프트 엔지니어링 (Prompt Engineering)
AI에게 업무 지시를 내리는 기술입니다. 헬스케어에서는 정확도가 생명입니다.

* **Zero-shot**: 예시 없음. ("진단해줘") -> 불안정함.
* **Few-shot**: 예시를 3~5개 제공. ("이런 증상은 A과, 저런 증상은 B과야. 자 이제 맞춰봐.") -> **비식별화 및 톤앤매너 유지에 필수.**
* **CoT (Chain of Thought)**: 풀이 과정을 예시로 줌. ("A증상이 있으니 B일 가능성이 높아. 따라서 결론은 C야.") -> 추론 능력 향상.

```python
# Few-shot Prompt 예시
prompt = \"\"\"
[예시1] 
입력: "배가 너무 아파요" 
출력: 소화기내과

[예시2]
입력: "눈이 침침해요"
출력: 안과

[실전]
입력: "무릎에서 소리가 나요"
출력: 
\"\"\"
```

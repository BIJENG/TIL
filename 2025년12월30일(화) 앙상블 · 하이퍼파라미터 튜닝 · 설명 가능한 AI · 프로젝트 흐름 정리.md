# 머신러닝 심화 학습 정리 (후반부 핵심)

> 앙상블 · 하이퍼파라미터 튜닝 · 설명 가능한 AI · 프로젝트 흐름 정리

---

## 1. 비지도 학습 II – 군집화 심화

### 1-1. Mean Shift
- **밀도(density)가 가장 높은 방향으로 데이터 이동**
- 군집 수 자동 결정
- bandwidth(h)에 따라 결과 크게 달라짐

**장점**
- 군집 개수 사전 지정 불필요

**단점**
- 계산 비용 큼
- 대용량 데이터에 비효율적

---

### 1-2. DBSCAN
- **밀도 기반 군집**
- eps 거리 안에 min_samples 이상이면 군집 형성
- 노이즈(outlier)를 자동 분리

**파라미터**
- eps: 이웃으로 판단하는 거리
- min_samples: 군집으로 인정받기 위한 최소 포인트 수

**특징**
- 군집 수 자동
- 이상치 탐지 가능
- 스케일링 필수

---

### 1-3. GMM (Gaussian Mixture Model)
- **확률 기반 군집 (Soft Clustering)**
- 데이터가 여러 가우시안 분포에서 생성되었다고 가정

**특징**
- 한 데이터가 여러 군집에 속할 확률 가짐
- components 수 직접 지정

---

### 군집 알고리즘 비교 요약

| 알고리즘 | 기준 | 군집 수 | 특징 |
|---|---|---|---|
| Mean Shift | 밀도 | 자동 | 중심 이동 |
| DBSCAN | 밀도 | 자동 | 노이즈 분리 |
| GMM | 확률 | 지정 | Soft Clustering |

---

## 2. Feature Engineering

### 정의
> **모델이 학습하기 쉬운 형태로 데이터를 재구성하는 과정**

### 중요성
- 알고리즘보다 성능에 더 큰 영향
- 노이즈 감소
- 의미 있는 패턴 강화

**예시**
- 단순 수치 → 범주화
- 비율 변수 생성
- 로그 변환

---

## 3. 교차 검증 (Cross Validation)

### 목적
- 데이터 분할 운빨 제거
- 모델 일반화 성능 평가

### K-Fold 구조
1. 데이터를 K개로 분할
2. 하나는 검증, 나머지는 학습
3. K번 반복
4. 평균 성능 사용

---

## 4. SVM (Support Vector Machine)

### 핵심 개념
- **마진(Margin)**: 두 클래스 사이 최대 간격
- **서포트 벡터**: 경계를 결정하는 핵심 데이터

### 특징
- 거리 기반 모델
- 스케일링 필수
- 소규모 데이터에서 강력

---

## 5. 앙상블(Ensemble) 학습

### 5-1. 배깅 (Bagging)
- 여러 모델을 **독립적으로 학습**
- 대표: Random Forest
- 분산 감소, 안정적

---

### 5-2. 부스팅 (Boosting)
- 이전 모델의 오류를 보완하며 순차 학습
- 대표 모델:
  - AdaBoost
  - Gradient Boosting
  - XGBoost
  - LightGBM
  - CatBoost

**특징**
- 성능 우수
- 과적합 주의
- learning_rate 중요

---

### 5-3. 스태킹 (Stacking)
- 여러 모델의 **예측값을 다시 학습**
- 메타 모델 사용
- 교차검증 기반으로 예측값 생성 필수

---

## 6. 하이퍼파라미터 최적화

### 왜 필요한가?
- 기본값은 최적이 아님
- 성능·과적합·속도에 영향

---

### 6-1. Grid Search
- 모든 조합 탐색
- 정확하지만 느림

---

### 6-2. Random Search
- 랜덤 탐색
- 빠르고 실무 친화적

---

### 6-3. Optuna
- 이전 실험 결과 기반 탐색
- 효율적 자동 튜닝
- Search Space 내에서 최적화

**핵심 개념**
- objective: 최적화 대상 함수
- trial: 하나의 실험 단위

---

## 7. Pipeline

### 정의
> **전처리 + 모델을 하나로 묶은 학습 흐름**

### 목적
- 데이터 누수 방지
- 교차검증 자동화
- 공정한 모델 비교

**구성**
- 전처리 (스케일링, 인코딩)
- 모델

---

## 8. SHAP (설명 가능한 AI)

### SHAP 값 의미
> **각 Feature가 예측값에 기여한 정도**

- 양수: 예측 증가
- 음수: 예측 감소
- baseline + SHAP 합 = 최종 예측

### 활용
- 변수 중요도 해석
- 개별 데이터(환자) 설명
- 모델 신뢰성 확보

---

## 9. 성능 평가와 공정 비교

### 모델 비교 원칙
- 동일 데이터
- 동일 CV
- 동일 평가 지표

### 트레이드오프
- 성능 ↑ → 해석 ↓
- 해석 ↑ → 성능 ↓

---

## 10. 심장 질환 예측 프로젝트 전체 흐름

1. 문제 정의
2. 데이터 이해 및 EDA
3. 전처리
4. Feature Engineering
5. 모델 학습
6. 교차검증
7. 하이퍼파라미터 튜닝
8. 성능 비교
9. SHAP 기반 해석
10. 결과 스토리 정리

---

## 📌 핵심 한 줄 요약

> **머신러닝 성능의 핵심은  
알고리즘이 아니라 데이터, 검증, 튜닝, 해석이다.**

## 도움 될 만한 사이트

- Storytell.ai = EDA 맛집 (데이터분석 맛집)
- pypi = 파이썬 패키지, 인덱스 다운받는곳

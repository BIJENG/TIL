# 딥러닝/퍼셉트론/역전파 핵심 정리 (수정·보완본)

---

## 1) 데이터가 “복잡하다”의 의미

### ✅ 데이터가 단순한 경우 (숫자로 표현이 쉬움)
- **표(tabular) 데이터**: 활력징후(혈압/맥박), 검사수치, 나이, 키/몸무게 등
- **범주형 데이터**도 숫자로 바꾸기 쉬움  
  - 예: 성별(M/F), 혈액형(A/B/O/AB) → **Label Encoding / One-Hot Encoding**

### ✅ 데이터가 복잡한 경우 (숫자로 “표현은 가능하지만” 차원이 매우 큼)
- **이미지**: 픽셀의 RGB 값 = 숫자이긴 한데 개수가 매우 많음  
  - 예: (500, 500, 3) ⇒ 500×500×3 = **750,000개** 숫자 (※ 75,000이 아니라 750,000)
- **자연어**: 문장을 “그대로”는 숫자로 표현하기 어려움  
  - 예: “통증이 심해졌다가 지금은 괜찮아졌다고 함”

---

## 2) 임베딩(Embedding) = “숫자 표현(Representation) + 의미를 담은 벡터”

### ✅ 임베딩이란?
- 단어/문장/이미지/환자기록 같은 데이터를 **벡터(숫자 리스트)** 로 바꾸는 것
- 좋은 임베딩은 “의미가 비슷하면 벡터도 가깝게” 만들어 줌  
  - 사랑 ↔ 애정 (가까움), 사랑 ↔ 키보드 (멀어짐)

### ✅ 포인트
- “자연어를 수치화 잘하는 게 LLM”이라는 표현은 방향은 맞지만,
  - 실제로는 **LLM 계열/Transformer 계열**에서 임베딩이 잘 나오고,
  - 실무에선 **임베딩 전용 모델**을 따로 쓰기도 함(검색/추천/유사도 등)

---

## 3) “문제가 복잡하다/단순하다”는 무슨 뜻?

### ✅ 직관적 기준
- **사람이 풀기 쉬운가?**  
  - 강아지/고양이 분류: 사람은 쉽다(분류 문제는 상대적으로 단순할 수 있음)
  - “치와와를 지브리 스타일로 그려줘”: 사람에게도 어렵다(생성 문제는 더 복잡)

### ✅ 더 중요한 기준(실전)
- 난이도는 “문제 자체” + “데이터 양/품질”로 결정됨  
  - 데이터 적은데 복잡한 문제(예: 희귀 질환 예측) → 매우 어려움  
  - 데이터 많고 라벨 정확하면 → 훨씬 풀기 쉬움

---

## 4) 규칙 기반 vs 머신러닝 vs 딥러닝 vs LLM (큰 그림)

### 규칙 기반(규칙을 사람이 설계)
- 장점: 예측 근거가 비교적 명확
- 한계: 패턴이 복잡해지면 규칙 설계가 사실상 불가능(이미지/자연어)

### 머신러닝(ML)
- 사람이 **특징(Feature)** 를 잘 뽑아주면 성능이 잘 나옴
- 예전 자연어 ML: BoW/TF-IDF처럼 “단어 빈도 기반”을 많이 사용  
  - 단점: 단어가 너무 많고(어휘 폭발), 의미/문맥 반영이 약함

### 딥러닝(DL)
- 특징 추출을 모델이 “학습”으로 상당 부분 해결(Representation Learning)
- 이미지/자연어 등 고차원 데이터에서 강함
- 단점: 해석(설명)이 어려워 **블랙박스**에 가까움

### LLM
- 매우 큰 딥러닝 모델(Transformer 기반)
- 생성/요약/대화/추론/코딩 등 “범용”에 강함

---

## 5) 딥러닝에서 “학습”의 정확한 뜻

### ✅ 학습 = 파라미터 업데이트
- 신경망은 입력에 대해
  - “곱하고 더하는 숫자들” = **파라미터(가중치 W, 편향 b)**
- 처음엔 랜덤이라 예측도 랜덤에 가깝다
- 학습은 파라미터를 **정답에 가까워지는 방향으로** 반복 업데이트하는 것

> 인간 공부 비유: 틀린 문제(오답)를 줄이도록 반복 학습

---

## 6) 손실함수(Loss Function) = 딥러닝의 “메타인지”

- 손실값이 크다 = 틀림이 많다
- 손실값이 작다 = 정답에 가깝다
- 목표: **손실을 최소화(minimize)** 하는 것  
  - “0으로 만든다”는 건 이상적인 목표(현실에선 0이 안 되는 경우가 많음)

### ✅ 예: 회귀의 대표 손실 (MSE)
- 예측: y_hat = f(x)
- MSE: 평균((y - y_hat)^2)

---

## 7) 순전파(Forward Pass) 쉽게 이해하기

### ✅ 한 층(layer)의 핵심
- **선형변환(가중합)**: `z = W x + b`
- **비선형변환(활성화)**: `h = activation(z)`

### ✅ 다층 퍼셉트론(MLP)
- (선형 → 비선형) 을 여러 번 반복
- 이 “반복” 덕분에 **복잡한 패턴**을 표현 가능

---

## 8) 활성화 함수(Activation)의 “진짜 역할” 2가지

### (1) 비선형성 추가 (표현력 증가)
- 활성화가 없으면 층을 여러 개 쌓아도 결국 “하나의 선형”으로 합쳐짐  
  → 그래서 **중간에 비선형이 꼭 필요**

### (2) 출력 형태 맞추기
- 이진 분류: sigmoid로 0~1 확률 형태
- 다중 분류(10클래스): softmax로 각 클래스 확률 분포 형태

---

## 9) “활성화 함수 조건” 보완 (중요)

✅ **(중요) 경사하강법으로 학습하려면**
- 활성화(및 전체 연산)가 **미분 가능하거나**, 최소한 **거의 모든 점에서 미분 가능**해야 함

📌 ReLU는 x=0에서 미분이 애매하지만(미분 불능),
- 딥러닝은 **서브그래디언트(subgradient)** 를 써서 학습 가능  
→ 그래서 ReLU 계열이 널리 쓰임

---

## 10) Sigmoid를 “안 쓰는 이유” 정확한 표현

- Sigmoid는 깊은 신경망의 **은닉층(hidden layer)** 에서 쓰면
  - 출력이 0~1로 눌리면서 기울기가 작아져 **기울기 소실(vanishing gradient)** 이 잘 발생
- 그래서 은닉층은 보통 **ReLU 계열**을 많이 사용

✅ 하지만 Sigmoid가 “완전히 안 쓰이는 건 아님”
- **이진 분류의 출력층(output layer)** 에서는 지금도 매우 자주 사용
- LSTM 같은 구조에서도 sigmoid가 중요한 역할을 함(게이트)

---

## 11) 역전파(Backward Pass) = 기울기(gradient) 계산 과정

### ✅ 역전파가 하는 일
- 손실 L이 있을 때,
- 각 파라미터(W, b)가 손실에 얼마나 영향을 주는지  
  → `dL/dW`, `dL/db` 를 계산

즉,
- **Backward Pass = 미분값(gradient) 계산**
- **Chain Rule(연쇄법칙)** 이 핵심

---

## 12) 경사하강법(Gradient Descent) = 실제 업데이트 규칙

업데이트는 이 한 줄:

- `W(t) = W(t-1) - lr * gradient`
- 여기서 **lr = learning rate = η(에타)**

### ✅ 학습률(lr)의 의미
- 너무 크면: 손실이 튀거나 발산
- 너무 작으면: 너무 느리게 학습

---

## 13) “전역 최적점 vs 지역 최적점” 보완

- 딥러닝에서는 “지역 최적점(local minima)” 뿐 아니라
  - **안장점(saddle point)** 같은 지형도 많음
- 실제 목표는 “완벽한 전역 최적”보다
  - **일반화 성능(새 데이터에서 잘 맞추기)** 가 더 중요

---

## 14) “Universal Approximator” 정확한 의미

- 충분히 큰 신경망은 다양한 함수를 **근사(approximate)** 할 수 있다
- 단, “학습이 항상 잘 된다/데이터 적어도 된다”는 뜻은 아님  
  - 데이터/정규화/최적화/구조/하이퍼파라미터가 중요

---

## 15) GPU가 중요한 이유 = 행렬곱의 병렬처리

- 딥러닝 연산의 대부분은 `W x` 같은 **행렬 곱**
- 행렬 곱은 병렬처리에 최적화 → GPU가 매우 강함
- “곱하고 더한다” = 결국 “큰 행렬 연산을 빠르게 한다”는 뜻

---

## 16) PyTorch 텐서(Tensor) 정리 (핵심만)

- PyTorch는 모든 데이터를 **Tensor** 로 다룸 (NumPy array의 딥러닝 버전 느낌)
- 텐서의 “차원”은 rank(축의 개수)로 이해하면 편함

예:
- 0차 텐서(스칼라): `3`
- 1차 텐서(벡터): `[2, 2.5, 9.4]`  → shape: (3,)
- 2차 텐서(행렬): `[[2,3,1],[3,4,2]]` → shape: (2,3)

✅ 딥러닝에서 자주 보는 형태
- 표 데이터(배치 포함): (batch, features)
- 이미지: (batch, channels, height, width)

---

## 17) 오늘 질문 포인트 정답 모음

- **η(에타) = learning rate(학습률)** 로 쓰는 경우가 많다
- **Epoch(에포크) = 전체 학습 데이터를 1번 전부 순회한 횟수**
- **ReLU가 Sigmoid보다 은닉층에서 많이 쓰이는 이유**
  - 깊은 모델에서 sigmoid는 기울기 소실이 쉽게 발생
  - ReLU 계열은 상대적으로 학습이 잘 진행됨

---

## 한 줄 요약 (진짜 핵심)
- 딥러닝 학습 = **손실함수(Loss)를 줄이도록 파라미터(W,b)를 업데이트**
- 역전파 = **기울기 계산**, 경사하강법 = **업데이트 실행**, 학습률 = **보폭**
- 복잡한 데이터(이미지/자연어)는 **임베딩/표현학습**이 핵심

---
